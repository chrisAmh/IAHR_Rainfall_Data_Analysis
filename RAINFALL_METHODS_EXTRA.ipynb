{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 2.094594967128594\n",
      "Root Mean Squared Error (RMSE): 6.76692473797948\n"
     ]
    }
   ],
   "source": [
    "# Two stations using correlation,compeleteness and distance\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "df = pd.read_csv('STATIONS.csv')\n",
    "\n",
    "# Convert 'Date' to datetime and set as index\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# Define function for weighted mean imputation\n",
    "def weighted_mean_imputation(df, target, neighbors, correlations, distances, completeness):\n",
    "    weights = [correlation / distance * comp for correlation, distance, comp in zip(correlations, distances, completeness)]\n",
    "    total_weight = sum(weights)\n",
    "    normalized_weights = [weight / total_weight for weight in weights]\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if np.isnan(row[target]):\n",
    "            weighted_values = [row[neighbor] * weight for neighbor, weight in zip(neighbors, normalized_weights) if not np.isnan(row[neighbor])]\n",
    "            if weighted_values:\n",
    "                df.at[index, target] = sum(weighted_values)\n",
    "\n",
    "# Impute missing values in 'Thorak Cemetery'\n",
    "weighted_mean_imputation(df, 'Thorak Cemetery', ['CSIRO', 'Darwin Airport'], [0.85, 0.79], [5, 9], [0.9, 1.0])\n",
    "\n",
    "# Evaluate the imputation (need actual known values, simulating missing data here as an example)\n",
    "known_values = df['Thorak Cemetery'].copy()  # Assume we know all the original values\n",
    "mask = np.random.rand(len(df)) < 0.1  # Randomly select 10% of the data\n",
    "df.loc[mask, 'Thorak Cemetery'] = np.nan  # Introduce missing values\n",
    "weighted_mean_imputation(df, 'Thorak Cemetery', ['CSIRO', 'Darwin Airport'], [0.85, 0.79], [5, 9], [0.9, 1.0])\n",
    "\n",
    "# Calculate MAE and RMSE for the imputed values\n",
    "mae = mean_absolute_error(known_values[mask], df['Thorak Cemetery'][mask])\n",
    "rmse = np.sqrt(mean_squared_error(known_values[mask], df['Thorak Cemetery'][mask]))\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thorak Cemetery      88.052345\n",
      "CSIRO                82.741332\n",
      "Northlake            55.965330\n",
      "Karama NT            24.456152\n",
      "Berrimah Research    58.268185\n",
      "Darwin Airport       99.991502\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('STATIONS_DATA.csv', parse_dates=['Date'])\n",
    "# df = df.dropna(subset=['Thorak Cemetery'])\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# Evaluate the imputation (need actual known values, simulating missing data here as an example)\n",
    "# known_values = df['Thorak Cemetery'].copy()  # Assume we know all the original values\n",
    "# mask = np.random.rand(len(df)) < 0.1  # Randomly select 10% of the data\n",
    "# df.loc[mask, 'Thorak Cemetery'] = np.nan  # Introduce missing values\n",
    "\n",
    "# Define distances to stations in order from Thorak Cemetery: 1km, 2km, 3km, 4km, 5km\n",
    "# distances = [4.8, 8, 5.5, 5.5, 5]\n",
    "# stations = [col for col in df.columns if col != 'Thorak Cemetery']  # Exclude Thorak Cemetery from the stations list\n",
    "\n",
    "# Calculate Spearman correlation between Thorak Cemetery and other stations\n",
    "# correlations = df.corr(method='spearman')['Thorak Cemetery'].drop('Thorak Cemetery')\n",
    "\n",
    "# Calculate completeness of data for each station\n",
    "completeness = df.notnull().mean() * 100  # Percentage of non-missing values for each column\n",
    "\n",
    "# # Calculate weights based on correlation, distance, and data completeness\n",
    "# weights = {station: (correlations[station] / distances[idx]) * (completeness[station] / 100) for idx, station in enumerate(stations)}\n",
    "\n",
    "# Normalize weights so they sum to 1\n",
    "# total_weight = sum(weights.values())\n",
    "# normalized_weights = {station: weight / total_weight for station, weight in weights.items()}\n",
    "\n",
    "# # Function to impute missing values with weighted average\n",
    "# def weighted_impute(row):\n",
    "#     if pd.isna(row['Thorak Cemetery']):\n",
    "#         available_data = {station: row[station] for station in stations if pd.notna(row[station])}\n",
    "#         if not available_data:\n",
    "#             return np.nan  # No data available at all for imputation\n",
    "#         elif len(available_data) == 1:\n",
    "#             # Only one station has data, use it as the imputed value\n",
    "#             return list(available_data.values())[0]\n",
    "#         else:\n",
    "#             # Calculate weighted sum using available data and normalized weights\n",
    "#             weighted_sum = sum(row[station] * normalized_weights[station] for station in available_data if station in normalized_weights)\n",
    "#             return weighted_sum if weighted_sum != 0 else np.nan\n",
    "#     else:\n",
    "#         return row['Thorak Cemetery']\n",
    "\n",
    "# # Apply the imputation\n",
    "# df['Thorak Cemetery'] = df.apply(weighted_impute, axis=1)\n",
    "\n",
    "# # Calculate MAE and RMSE for the imputed values, excluding NaNs\n",
    "# imputed_values = df['Thorak Cemetery'][mask]\n",
    "# known_values = known_values[mask]\n",
    "\n",
    "# # Filter out NaN values from known_values and imputed_values\n",
    "# valid_mask = known_values.notna() & imputed_values.notna()\n",
    "# mae = mean_absolute_error(known_values[valid_mask], imputed_values[valid_mask])\n",
    "# rmse = np.sqrt(mean_squared_error(known_values[valid_mask], imputed_values[valid_mask]))\n",
    "\n",
    "# # Print normalized weights for review\n",
    "# print(\"Normalized Weights based on Spearman correlation, distances, and completeness:\", normalized_weights)\n",
    "# print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "# print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(completeness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Weights based on Spearman correlation and distances: {'CSIRO': 0.24968832488343082, 'Northlake': 0.14135490114681093, 'Karama NT': 0.18392302599072977, 'Berrimah Research': 0.20113841758797524, 'Darwin Airport': 0.22389533039105328}\n",
      "Mean Absolute Error (MAE): 6.684115017119337\n",
      "Root Mean Squared Error (RMSE): 11.646617079351191\n"
     ]
    }
   ],
   "source": [
    "# consider all stations with correlations with no completeness\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('STATIONS_DATA.csv', parse_dates=['Date'])\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# Evaluate the imputation (need actual known values, simulating missing data here as an example)\n",
    "known_values = df['Thorak Cemetery'].copy()  # Assume we know all the original values\n",
    "mask = np.random.rand(len(df)) < 0.1  # Randomly select 10% of the data\n",
    "df.loc[mask, 'Thorak Cemetery'] = np.nan  # Introduce missing values\n",
    "\n",
    "# Define distances to stations in order from Thorak Cemetery: 1km, 2km, 3km, 4km, 5km\n",
    "distances = [4.8, 8, 5.5, 5.5, 5]\n",
    "stations = [col for col in df.columns if col != 'Thorak Cemetery']  # Exclude Thorak Cemetery from the stations list\n",
    "\n",
    "# Calculate Spearman correlation between Thorak Cemetery and other stations\n",
    "correlations = df.corr(method='spearman')['Thorak Cemetery'].drop('Thorak Cemetery')\n",
    "\n",
    "# Calculate weights based on correlation and distance\n",
    "weights = {station: (correlations[station] / distances[idx]) for idx, station in enumerate(stations)}\n",
    "\n",
    "# Normalize weights so they sum to 1\n",
    "total_weight = sum(weights.values())\n",
    "normalized_weights = {station: weight / total_weight for station, weight in weights.items()}\n",
    "\n",
    "# Function to impute missing values with weighted average\n",
    "def weighted_impute(row):\n",
    "    if pd.isna(row['Thorak Cemetery']):\n",
    "        available_data = {station: row[station] for station in stations if pd.notna(row[station])}\n",
    "        if not available_data:\n",
    "            return np.nan  # No data available at all for imputation\n",
    "        elif len(available_data) == 1:\n",
    "            # Only one station has data, use it as the imputed value\n",
    "            return list(available_data.values())[0]\n",
    "        else:\n",
    "            # Calculate weighted sum using available data and normalized weights\n",
    "            weighted_sum = sum(row[station] * normalized_weights[station] for station in available_data if station in normalized_weights)\n",
    "            return weighted_sum if weighted_sum != 0 else np.nan\n",
    "    else:\n",
    "        return row['Thorak Cemetery']\n",
    "\n",
    "# Apply the imputation\n",
    "df['Thorak Cemetery'] = df.apply(weighted_impute, axis=1)\n",
    "\n",
    "# Calculate MAE and RMSE for the imputed values, excluding NaNs\n",
    "imputed_values = df['Thorak Cemetery'][mask]\n",
    "known_values = known_values[mask]\n",
    "\n",
    "# Filter out NaN values from known_values and imputed_values\n",
    "valid_mask = known_values.notna() & imputed_values.notna()\n",
    "mae = mean_absolute_error(known_values[valid_mask], imputed_values[valid_mask])\n",
    "rmse = np.sqrt(mean_squared_error(known_values[valid_mask], imputed_values[valid_mask]))\n",
    "\n",
    "# Print normalized weights for review\n",
    "print(\"Normalized Weights based on Spearman correlation and distances:\", normalized_weights)\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Weights based on Spearman correlation and distances: {'Darwin Airport': 0.23716672357371507, 'CSIRO': 0.4213235999011493, 'Berrimah Research': 0.34150967652513564}\n",
      "Mean Absolute Error (MAE): 6.54380286177368\n",
      "Root Mean Squared Error (RMSE): 11.902872587504927\n"
     ]
    }
   ],
   "source": [
    "# consider stations with correlations with no completeness\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('STATIONS_DATA.csv', parse_dates=['Date'])\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# Evaluate the imputation (need actual known values, simulating missing data here as an example)\n",
    "known_values = df['Thorak Cemetery'].copy()  # Assume we know all the original values\n",
    "mask = np.random.rand(len(df)) < 0.1  # Randomly select 10% of the data\n",
    "df.loc[mask, 'Thorak Cemetery'] = np.nan  # Introduce missing values\n",
    "\n",
    "# Define the distances for Darwin Airport, CSIRO, and Berrimah\n",
    "stations = ['Darwin Airport', 'CSIRO', 'Berrimah Research']  # List the specific stations to consider\n",
    "distances = [8, 4.8, 5.5]  # Corresponding distances to Thorak Cemetery\n",
    "\n",
    "# Calculate Spearman correlation between Thorak Cemetery and the specified stations\n",
    "correlations = df[stations].corrwith(df['Thorak Cemetery'], method='spearman')\n",
    "\n",
    "# Calculate weights based on correlation and distance\n",
    "weights = {station: (correlations[station] / distances[idx]) for idx, station in enumerate(stations)}\n",
    "\n",
    "# Normalize weights so they sum to 1\n",
    "total_weight = sum(weights.values())\n",
    "normalized_weights = {station: weight / total_weight for station, weight in weights.items()}\n",
    "\n",
    "# Function to impute missing values with weighted average\n",
    "def weighted_impute(row):\n",
    "    if pd.isna(row['Thorak Cemetery']):\n",
    "        available_data = {station: row[station] for station in stations if pd.notna(row[station])}\n",
    "        if not available_data:\n",
    "            return np.nan  # No data available at all for imputation\n",
    "        elif len(available_data) == 1:\n",
    "            # Only one station has data, use it as the imputed value\n",
    "            return list(available_data.values())[0]\n",
    "        else:\n",
    "            # Calculate weighted sum using available data and normalized weights\n",
    "            weighted_sum = sum(row[station] * normalized_weights[station] for station in available_data if station in normalized_weights)\n",
    "            return weighted_sum if weighted_sum != 0 else np.nan\n",
    "    else:\n",
    "        return row['Thorak Cemetery']\n",
    "\n",
    "# Apply the imputation\n",
    "df['Thorak Cemetery'] = df.apply(weighted_impute, axis=1)\n",
    "\n",
    "# Calculate MAE and RMSE for the imputed values, excluding NaNs\n",
    "imputed_values = df['Thorak Cemetery'][mask]\n",
    "known_values = known_values[mask]\n",
    "\n",
    "# Filter out NaN values from known_values and imputed_values\n",
    "valid_mask = known_values.notna() & imputed_values.notna()\n",
    "mae = mean_absolute_error(known_values[valid_mask], imputed_values[valid_mask])\n",
    "rmse = np.sqrt(mean_squared_error(known_values[valid_mask], imputed_values[valid_mask]))\n",
    "\n",
    "# Print normalized weights for review\n",
    "print(\"Normalized Weights based on Spearman correlation and distances:\", normalized_weights)\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Weights based on Spearman correlation, distances, and completeness: {'Darwin Airport': 0.3019199793497001, 'CSIRO': 0.4448852861437701, 'Berrimah Research': 0.25319473450652974}\n",
      "Mean Absolute Error (MAE): 7.507824975039637\n",
      "Root Mean Squared Error (RMSE): 14.556227127040454\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('STATIONS_DATA.csv', parse_dates=['Date'])\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# Evaluate the imputation (need actual known values, simulating missing data here as an example)\n",
    "known_values = df['Thorak Cemetery'].copy()  # Assume we know all the original values\n",
    "mask = np.random.rand(len(df)) < 0.1  # Randomly select 10% of the data\n",
    "df.loc[mask, 'Thorak Cemetery'] = np.nan  # Introduce missing values\n",
    "\n",
    "# Define the distances for Darwin Airport, CSIRO, and Berrimah\n",
    "stations = ['Darwin Airport', 'CSIRO', 'Berrimah Research']  # List the specific stations to consider\n",
    "distances = [8, 4.8, 5.5]  # Corresponding distances to Thorak Cemetery\n",
    "\n",
    "# Calculate Spearman correlation between Thorak Cemetery and the specified stations\n",
    "correlations = df[stations].corrwith(df['Thorak Cemetery'], method='spearman')\n",
    "\n",
    "# Calculate completeness of data for each station\n",
    "completeness = df[stations].notnull().mean() * 100  # Percentage of non-missing values for each specified station\n",
    "\n",
    "# Calculate weights based on correlation, distance, and data completeness\n",
    "weights = {station: (correlations[station] / distances[idx]) * (completeness[station] / 100) for idx, station in enumerate(stations)}\n",
    "\n",
    "# Normalize weights so they sum to 1\n",
    "total_weight = sum(weights.values())\n",
    "normalized_weights = {station: weight / total_weight for station, weight in weights.items()}\n",
    "\n",
    "# Function to impute missing values with weighted average\n",
    "def weighted_impute(row):\n",
    "    if pd.isna(row['Thorak Cemetery']):\n",
    "        available_data = {station: row[station] for station in stations if pd.notna(row[station])}\n",
    "        if not available_data:\n",
    "            return np.nan  # No data available at all for imputation\n",
    "        elif len(available_data) == 1:\n",
    "            # Only one station has data, use it as the imputed value\n",
    "            return list(available_data.values())[0]\n",
    "        else:\n",
    "            # Calculate weighted sum using available data and normalized weights\n",
    "            weighted_sum = sum(row[station] * normalized_weights[station] for station in available_data if station in normalized_weights)\n",
    "            return weighted_sum if weighted_sum != 0 else np.nan\n",
    "    else:\n",
    "        return row['Thorak Cemetery']\n",
    "\n",
    "# Apply the imputation\n",
    "df['Thorak Cemetery'] = df.apply(weighted_impute, axis=1)\n",
    "\n",
    "# Calculate MAE and RMSE for the imputed values, excluding NaNs\n",
    "imputed_values = df['Thorak Cemetery'][mask]\n",
    "known_values = known_values[mask]\n",
    "\n",
    "# Filter out NaN values from known_values and imputed_values\n",
    "valid_mask = known_values.notna() & imputed_values.notna()\n",
    "mae = mean_absolute_error(known_values[valid_mask], imputed_values[valid_mask])\n",
    "rmse = np.sqrt(mean_squared_error(known_values[valid_mask], imputed_values[valid_mask]))\n",
    "\n",
    "# Print normalized weights for review\n",
    "print(\"Normalized Weights based on Spearman correlation, distances, and completeness:\", normalized_weights)\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Weights based on Spearman correlation, distances, and completeness: {'Darwin Airport': 0.4046753929882326, 'CSIRO': 0.5953246070117674}\n",
      "Mean Absolute Error (MAE): 6.4001701481632844\n",
      "Root Mean Squared Error (RMSE): 13.355028299555359\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('STATIONS_DATA.csv', parse_dates=['Date'])\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# Evaluate the imputation (need actual known values, simulating missing data here as an example)\n",
    "known_values = df['Thorak Cemetery'].copy()  # Assume we know all the original values\n",
    "mask = np.random.rand(len(df)) < 0.1  # Randomly select 10% of the data\n",
    "df.loc[mask, 'Thorak Cemetery'] = np.nan  # Introduce missing values\n",
    "\n",
    "# Define the distances for Darwin Airport and CSIRO\n",
    "stations = ['Darwin Airport', 'CSIRO']  # List the specific stations to consider\n",
    "distances = [8, 4.8]  # Corresponding distances to Thorak Cemetery\n",
    "\n",
    "# Calculate Spearman correlation between Thorak Cemetery and the specified stations\n",
    "correlations = df[stations].corrwith(df['Thorak Cemetery'], method='spearman')\n",
    "\n",
    "# Calculate completeness of data for each station\n",
    "completeness = df[stations].notnull().mean() * 100  # Percentage of non-missing values for each specified station\n",
    "\n",
    "# Calculate weights based on correlation, distance, and data completeness\n",
    "weights = {station: (correlations[station] / distances[idx]) * (completeness[station] / 100) for idx, station in enumerate(stations)}\n",
    "\n",
    "# Normalize weights so they sum to 1\n",
    "total_weight = sum(weights.values())\n",
    "normalized_weights = {station: weight / total_weight for station, weight in weights.items()}\n",
    "\n",
    "# Function to impute missing values with weighted average\n",
    "def weighted_impute(row):\n",
    "    if pd.isna(row['Thorak Cemetery']):\n",
    "        available_data = {station: row[station] for station in stations if pd.notna(row[station])}\n",
    "        if not available_data:\n",
    "            return np.nan  # No data available at all for imputation\n",
    "        elif len(available_data) == 1:\n",
    "            # Only one station has data, use it as the imputed value\n",
    "            return list(available_data.values())[0]\n",
    "        else:\n",
    "            # Calculate weighted sum using available data and normalized weights\n",
    "            weighted_sum = sum(row[station] * normalized_weights[station] for station in available_data if station in normalized_weights)\n",
    "            return weighted_sum if weighted_sum != 0 else np.nan\n",
    "    else:\n",
    "        return row['Thorak Cemetery']\n",
    "\n",
    "# Apply the imputation\n",
    "df['Thorak Cemetery'] = df.apply(weighted_impute, axis=1)\n",
    "\n",
    "# Calculate MAE and RMSE for the imputed values, excluding NaNs\n",
    "imputed_values = df['Thorak Cemetery'][mask]\n",
    "known_values = known_values[mask]\n",
    "\n",
    "# Filter out NaN values from known_values and imputed_values\n",
    "valid_mask = known_values.notna() & imputed_values.notna()\n",
    "mae = mean_absolute_error(known_values[valid_mask], imputed_values[valid_mask])\n",
    "rmse = np.sqrt(mean_squared_error(known_values[valid_mask], imputed_values[valid_mask]))\n",
    "\n",
    "# Print normalized weights for review\n",
    "print(\"Normalized Weights based on Spearman correlation, distances, and completeness:\", normalized_weights)\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Weights based on Spearman correlation, distances, and completeness: {'Darwin Airport': 0.404122481011266, 'CSIRO': 0.595877518988734}\n",
      "Mean Absolute Error (MAE): 5.709640999887143\n",
      "Root Mean Squared Error (RMSE): 11.887565195813679\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('STATIONS.csv', parse_dates=['Date'])\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# Evaluate the imputation (need actual known values, simulating missing data here as an example)\n",
    "known_values = df['Thorak Cemetery'].copy()  # Assume we know all the original values\n",
    "mask = np.random.rand(len(df)) < 0.1  # Randomly select 10% of the data\n",
    "df.loc[mask, 'Thorak Cemetery'] = np.nan  # Introduce missing values\n",
    "\n",
    "# Define the distances for Darwin Airport and CSIRO\n",
    "stations = ['Darwin Airport', 'CSIRO']  # List the specific stations to consider\n",
    "distances = [8, 4.8]  # Corresponding distances to Thorak Cemetery\n",
    "\n",
    "# Calculate Spearman correlation between Thorak Cemetery and the specified stations\n",
    "correlations = df[stations].corrwith(df['Thorak Cemetery'], method='spearman')\n",
    "\n",
    "# Calculate completeness of data for each station\n",
    "completeness = df[stations].notnull().mean() * 100  # Percentage of non-missing values for each specified station\n",
    "\n",
    "# Calculate weights based on correlation, distance, and data completeness\n",
    "weights = {station: (correlations[station] / distances[idx]) * (completeness[station] / 100) for idx, station in enumerate(stations)}\n",
    "\n",
    "# Normalize weights so they sum to 1\n",
    "total_weight = sum(weights.values())\n",
    "normalized_weights = {station: weight / total_weight for station, weight in weights.items()}\n",
    "\n",
    "# Function to impute missing values with weighted average\n",
    "def weighted_impute(row):\n",
    "    if pd.isna(row['Thorak Cemetery']):\n",
    "        available_data = {station: row[station] for station in stations if pd.notna(row[station])}\n",
    "        if not available_data:\n",
    "            return np.nan  # No data available at all for imputation\n",
    "        elif len(available_data) == 1:\n",
    "            # Only one station has data, use it as the imputed value\n",
    "            return list(available_data.values())[0]\n",
    "        else:\n",
    "            # Calculate weighted sum using available data and normalized weights\n",
    "            weighted_sum = sum(row[station] * normalized_weights[station] for station in available_data if station in normalized_weights)\n",
    "            return weighted_sum if weighted_sum != 0 else np.nan\n",
    "    else:\n",
    "        return row['Thorak Cemetery']\n",
    "\n",
    "# Apply the imputation\n",
    "df['Thorak Cemetery'] = df.apply(weighted_impute, axis=1)\n",
    "\n",
    "# Calculate MAE and RMSE for the imputed values, excluding NaNs\n",
    "imputed_values = df['Thorak Cemetery'][mask]\n",
    "known_values = known_values[mask]\n",
    "\n",
    "# Filter out NaN values from known_values and imputed_values\n",
    "valid_mask = known_values.notna() & imputed_values.notna()\n",
    "mae = mean_absolute_error(known_values[valid_mask], imputed_values[valid_mask])\n",
    "rmse = np.sqrt(mean_squared_error(known_values[valid_mask], imputed_values[valid_mask]))\n",
    "\n",
    "# Print normalized weights for review\n",
    "print(\"Normalized Weights based on Spearman correlation, distances, and completeness:\", normalized_weights)\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Weights based on Spearman correlation, distances, and completeness: {'Darwin Airport': 0.40438082216718024, 'CSIRO': 0.5956191778328198}\n",
      "Mean Absolute Error (MAE): 5.603738849121125\n",
      "Root Mean Squared Error (RMSE): 11.265911078752266\n",
      "Example MAE values: 2 and 5\n",
      "Example RMSE values: 6 and 11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('STATIONS.csv', parse_dates=['Date'])\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# Define the distances for Darwin Airport and CSIRO\n",
    "stations = ['Darwin Airport', 'CSIRO']  # List the specific stations to consider\n",
    "distances = [8, 4.8]  # Corresponding distances to Thorak Cemetery\n",
    "\n",
    "# Calculate Spearman correlation between Thorak Cemetery and the specified stations\n",
    "correlations = df[stations].corrwith(df['Thorak Cemetery'], method='spearman')\n",
    "\n",
    "# Calculate completeness of data for each station\n",
    "completeness = df[stations].notnull().mean()  # Fraction of non-missing values for each specified station\n",
    "\n",
    "# Calculate weights based on correlation, distance, and data completeness\n",
    "weights = {station: (correlations[station] / distances[idx]) * completeness[station] for idx, station in enumerate(stations)}\n",
    "\n",
    "# Normalize weights so they sum to 1\n",
    "total_weight = sum(weights.values())\n",
    "normalized_weights = {station: weight / total_weight for station, weight in weights.items()}\n",
    "\n",
    "# Function to impute missing values with weighted average\n",
    "def weighted_impute(row):\n",
    "    if pd.isna(row['Thorak Cemetery']):\n",
    "        available_data = {station: row[station] for station in stations if pd.notna(row[station])}\n",
    "        if not available_data:\n",
    "            return np.nan  # No data available at all for imputation\n",
    "        elif len(available_data) == 1:\n",
    "            # Only one station has data, use it as the imputed value\n",
    "            return list(available_data.values())[0]\n",
    "        else:\n",
    "            # Calculate weighted sum using available data and normalized weights\n",
    "            weighted_sum = sum(row[station] * normalized_weights[station] for station in available_data if station in normalized_weights)\n",
    "            return weighted_sum if weighted_sum != 0 else np.nan\n",
    "    else:\n",
    "        return row['Thorak Cemetery']\n",
    "\n",
    "# Evaluate the imputation (need actual known values, simulating missing data here as an example)\n",
    "known_values = df['Thorak Cemetery'].copy()  # Assume we know all the original values\n",
    "mask = np.random.rand(len(df)) < 0.1  # Randomly select 10% of the data\n",
    "df.loc[mask, 'Thorak Cemetery'] = np.nan  # Introduce missing values\n",
    "\n",
    "# Apply the imputation\n",
    "df['Thorak Cemetery'] = df.apply(weighted_impute, axis=1)\n",
    "\n",
    "# Calculate MAE and RMSE for the imputed values, excluding NaNs\n",
    "imputed_values = df['Thorak Cemetery'][mask]\n",
    "known_values = known_values[mask]\n",
    "\n",
    "# Filter out NaN values from known_values and imputed_values\n",
    "valid_mask = known_values.notna() & imputed_values.notna()\n",
    "mae = mean_absolute_error(known_values[valid_mask], imputed_values[valid_mask])\n",
    "rmse = np.sqrt(mean_squared_error(known_values[valid_mask], imputed_values[valid_mask]))\n",
    "\n",
    "# Print normalized weights for review\n",
    "print(\"Normalized Weights based on Spearman correlation, distances, and completeness:\", normalized_weights)\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# Given example values\n",
    "print(\"Example MAE values: 2 and 5\")\n",
    "print(\"Example RMSE values: 6 and 11\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
